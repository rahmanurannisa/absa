{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBd-AgBZNk0V"
      },
      "source": [
        "<center><h2><strong><font color=\"blue\">Social Network Analysis (SNA)</font></strong></h2></center>\n",
        "<center><h3><strong><font color=\"blue\"><a href=\"https://taudata.blogspot.com\">https://taudata.blogspot.com</a></font></strong></h3></center>\n",
        "\n",
        "<img alt=\"\" src=\"https://github.com/taudataid/eLearning/blob/master/images/covers/taudata-cover.jpg?raw=1\"/>\n",
        "\n",
        "<center><h2><strong><font color=\"blue\">NLPTM-05: Pendahuluan Representasi Teks/Dokumen (VSM & Word Embedding)</font></strong></h2></center>\n",
        "<center><h3><strong><font color=\"blue\"><a href=\"https://taudata.blogspot.com/2022/04/nlptm-05.html\">https://taudata.blogspot.com/2022/04/nlptm-05.html</a></font></strong></h3></center>\n",
        "<b><center><h3>(C) Taufik Sutanto</h3></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfmcOGQKNk0a",
        "outputId": "a75897f8-2e60-4543-dbf9-bdd79edb85dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\n"
          ]
        }
      ],
      "source": [
        "# Installing Modules for Google Colab\n",
        "import warnings, nltk; warnings.simplefilter('ignore')\n",
        "\n",
        "try:\n",
        "    import google.colab; IN_COLAB = True\n",
        "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
        "    !mkdir data\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/Tweets_example.json\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
        "\n",
        "    !pip install spacy python-crfsuite unidecode textblob sastrawi tweepy twython\n",
        "    !python -m spacy download xx\n",
        "    !python -m spacy download en_core_web_sm\n",
        "\n",
        "    nltk.download('popular')\n",
        "    print(\"Running the code in colab, don't forget to change the runtime (if needed)\")    \n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKFcs6SPNk0c"
      },
      "outputs": [],
      "source": [
        "import taudataNlpTm as tau, seaborn as sns; sns.set()\n",
        "import tweepy, json, nltk, urllib.request\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from tqdm import tqdm_notebook as tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hciiwoXoNk0d"
      },
      "source": [
        "# pertama-tama mari kita Load Data twitter dari pertemuan sebelumnya\n",
        "\n",
        "* Silahkan gunakan data baru (crawl lagi) jika diinginkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivEZNMtMNk0d"
      },
      "outputs": [],
      "source": [
        "def loadTweets(file='Tweets.json'):\n",
        "    f=open(file,encoding='utf-8', errors ='ignore', mode='r')\n",
        "    T=f.readlines();f.close()\n",
        "    for i,t in enumerate(T):\n",
        "        T[i] = json.loads(t.strip())\n",
        "    return T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSLdeK1JNk0d",
        "outputId": "53277d52-fba1-4f5b-dc30-999d54b17d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total data = 10000\n",
            "tweet pertama oleh \"Nuy_indra\" : \"RT @aiceindonesia: Semoga dengan adanya aksi sosial ini masyarakat Indonesia makin disiplin memakai masker, patuhi protokol kesehatan, danâ€¦\"\n"
          ]
        }
      ],
      "source": [
        "# karena ToS data json ini dikirimkan terpisah hanya untuk kalangan terbatas.\n",
        "\n",
        "T2 = loadTweets(file='data/Tweets_example.json')\n",
        "print('Total data = {}'.format(len(T2)))\n",
        "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5wKyTzuNk0e",
        "outputId": "c34b8e6b-3aa3-4715-d3ea-e82cbc775934"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RT @aiceindonesia: Semoga dengan adanya aksi sosial ini masyarakat Indonesia makin disiplin memakai masker, patuhi protokol kesehatan, danâ€¦',\n",
              " 'RT @ecosocrights: Bukan hanya konglomerat, kekayaan para pejabat juga naik selama pandemi, termasuk kekayaan Presiden. Pandemi menguntungkaâ€¦',\n",
              " 'siapa diantara temen temen disini yg kekayaannya selama pandemi naik 70%?',\n",
              " 'RT @Gamal_Albinsaid: Tercatat belasan ribu anak menjadi yatim piatu karena pandemi COVID-19.\\n\\nIni memotivasi saya mengembangkan program Anaâ€¦',\n",
              " 'RT @kyumbin131: âœ¨ 22nd Junkyu Birthday Project ðŸ‡®ðŸ‡© âœ¨\\nPenyaluran 22 paket sembako utk keluarga/masyarakat yg kesulitan di masa pandemi.\\n\\nSemoâ€¦']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Contoh mengambil hanya data tweet\n",
        "data = [t['full_text'] for t in T2]\n",
        "data[:5] # 5 tweet pertama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNONeEGoNk0e"
      },
      "source": [
        "# PreProcessing Data Text-nya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3aB64t-Nk0f",
        "outputId": "30c412e6-4668-4d9b-d4b4-d87094c7749e",
        "colab": {
          "referenced_widgets": [
            "180668bca3d042beacf571bbca31011f"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "180668bca3d042beacf571bbca31011f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aiceindonesia moga aksi sosial masyarakat indonesia disiplin pakai masker patuh protokol sehat\n"
          ]
        }
      ],
      "source": [
        "# pre processing\n",
        "\n",
        "stops, lemmatizer = tau.LoadStopWords(lang='id')\n",
        "stops.add('rt')\n",
        "stops.add('..')\n",
        "for i,d in tqdm(enumerate(data)):\n",
        "    data[i] = tau.cleanText(d, lemma=lemmatizer, stops = stops, symbols_remove = True, min_charLen = 2)\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ0Cv_WrNk0f"
      },
      "outputs": [],
      "source": [
        "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
        "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6AMWT0TNk0f",
        "outputId": "b60588f1-b935-439e-b696-8b97fdcee8c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 9851)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# VSM - \"binari\"\n",
        "binary_vectorizer = CountVectorizer(binary = True)\n",
        "binari = binary_vectorizer.fit_transform(data)\n",
        "binari.shape # ukuran VSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_1y1WobNk0g",
        "outputId": "1b67b342-2c70-40ea-d03e-051ad875775d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x9851 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 12 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sparse vectors/matrix\n",
        "binari[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YixGOLfNk0g",
        "outputId": "14c0ecd9-ed11-4c1d-8923-4985e17c4570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[ 141 5623  186 8381 5299 3493 2048 6503 5279 6644 7158 7909]\n"
          ]
        }
      ],
      "source": [
        "# Mengakses Datanya\n",
        "print(binari[0].data)\n",
        "print(binari[0].indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jo49FwlNk0g",
        "outputId": "39032bb9-e3d0-4f42-aa6b-fc28c4d686fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'aiceindonesia': 141, 'moga': 5623, 'aksi': 186, 'sosial': 8381, 'masyarakat': 5299, 'indone\n"
          ]
        }
      ],
      "source": [
        "# Kolom dan term\n",
        "print(str(binary_vectorizer.vocabulary_)[:93])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T82QO2tFNk0h",
        "outputId": "d4a3e409-5737-4f98-8a5c-4c7dbd829473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 9851)\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[ 141 5623  186 8381 5299 3493 2048 6503 5279 6644 7158 7909]\n"
          ]
        }
      ],
      "source": [
        "# VSM term Frekuensi : \"tf\"\n",
        "tf_vectorizer = CountVectorizer(binary = False)\n",
        "tf = tf_vectorizer.fit_transform(data)\n",
        "\n",
        "print(tf.shape) # Sama\n",
        "print(tf[0].data) # Hanya data ini yg berubah\n",
        "print(tf[0].indices) # Letak kolomnya tetap sama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVOsX6b5Nk0h",
        "outputId": "ae9e026d-1e89-4231-bb25-b268706c6317"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'awkward'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d = tf_vectorizer.vocabulary_\n",
        "kata_kolom = {k:v for v,k in d.items()}\n",
        "kata_kolom[597]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrK-DKQZNk0h",
        "outputId": "c57f03b4-9aa2-4744-c97e-7e2eb64c3e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 9851)\n",
            "[0.24587303 0.30198705 0.33266445 0.27506537 0.29841056 0.31648834\n",
            " 0.21882555 0.19953934 0.25019267 0.3566293  0.23883859 0.37186648]\n",
            "[7909 7158 6644 5279 6503 2048 3493 5299 8381  186 5623  141]\n"
          ]
        }
      ],
      "source": [
        "# VSM term Frekuensi : \"tf-idf\"\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(data)\n",
        "\n",
        "print(tfidf.shape) # Sama\n",
        "print(tfidf[0].data) # Hanya data ini yg berubah\n",
        "print(tfidf[0].indices) # Letak kolomnya berbeda, namun jumlah kolom dan elemennya tetap sama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5T_zTE9Nk0i",
        "outputId": "2c78d040-369b-4781-f389-352b1ee32a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 9851)\n",
            "[0.24525912 0.3016796  0.3328828  0.27454194 0.29806418 0.31638117\n",
            " 0.21820422 0.19893964 0.24958561 0.3576212  0.2382173  0.37361798]\n",
            "[7909 7158 6644 5279 6503 2048 3493 5299 8381  186 5623  141]\n"
          ]
        }
      ],
      "source": [
        "# VSM term Frekuensi : \"tf-idf\"\n",
        "tfidf_vectorizer = TfidfVectorizer(smooth_idf= False, sublinear_tf=True)\n",
        "tfidf = tfidf_vectorizer.fit_transform(data)\n",
        "print(tfidf.shape) # Sama\n",
        "print(tfidf[0].data) # Hanya data ini yg berubah\n",
        "print(tfidf[0].indices) # Letak kolomnya = tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nIXt7fjNk0i",
        "outputId": "25f6b06d-8c04-49c7-a066-f86fb6825353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 9851)\n",
            "(10000, 2382)\n"
          ]
        }
      ],
      "source": [
        "# Frequency Filtering di VSM\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_1 = tfidf_vectorizer.fit_transform(data)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=5)\n",
        "tfidf_2 = tfidf_vectorizer.fit_transform(data)\n",
        "\n",
        "print(tfidf_1.shape)\n",
        "print(tfidf_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ0Tgul9Nk0i",
        "outputId": "7e860b04-8626-4809-8a8c-4d1ceec73abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 12312)\n"
          ]
        }
      ],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(lowercase=True, smooth_idf= True, sublinear_tf=True, \n",
        "                                   ngram_range=(1, 2), max_df=0.90, min_df=2)\n",
        "\n",
        "tfidf_3 = tfidf_vectorizer.fit_transform(data)\n",
        "print(tfidf_3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxPRPD7aNk0j"
      },
      "source": [
        "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
        "\n",
        "<p><img alt=\"\" src=\"https://github.com/taudataid/eLearning/blob/master/images/3_bm25_simple.png?raw=1\" style=\"height: 123px; width: 300px;\" /></p>\n",
        "\n",
        "<ol>\n",
        "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
        "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
        "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
        "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
        "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
        "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXMnu-JONk0j",
        "outputId": "1441fe94-7e87-4838-deb9-3a0fa9964e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'udin76': 35, 'minum': 21, 'kopi': 14, 'pagi': 24, 'sambil': 28, 'makan': 18, 'pisang': 25, 'goreng': 11, 'is': 12, 'the': 33, 'best': 6, 'belajar': 4, 'nlp': 22, 'dan': 8, 'text': 32, 'mining': 20, 'ternyata': 31, 'seru': 29, 'banget': 3, 'sadiezz': 27, 'sudah': 30, 'lumayan': 17, 'lama': 15, 'bingits': 7, 'tukang': 34, 'bakso': 2, 'belum': 5, 'lewat': 16, 'aduh': 0, 'ga': 10, 'mie': 19, 'ayam': 1, 'p4k4i': 23, 'kesyap': 13, 'please': 26, 'deh': 9}\n"
          ]
        }
      ],
      "source": [
        "# Variasi pembentukan matriks VSM:\n",
        "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
        "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
        "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
        "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
        "\n",
        "D = [d1, d2, d3, d4]\n",
        "# Jika kita menggunakan cara biasa: \n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "vsm = tfidf_vectorizer.fit_transform(D)\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhs3z1NrNk0j",
        "outputId": "5bfa37e3-92b2-49cb-b257-04bf65cff995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'udin76': 69, 'minum': 41, 'kopi': 27, 'pagi': 47, 'sambil': 55, 'makan': 34, 'pisang': 50, 'goreng': 21, 'is': 23, 'the': 65, 'best': 13, 'udin76 minum': 70, 'minum kopi': 42, 'kopi pagi': 28, 'pagi pagi': 48, 'pagi sambil': 49, 'sambil makan': 56, 'makan pisang': 36, 'pisang goreng': 51, 'goreng is': 22, 'is the': 24, 'the best': 66, 'belajar': 9, 'nlp': 43, 'dan': 16, 'text': 63, 'mining': 39, 'ternyata': 61, 'seru': 57, 'banget': 6, 'sadiezz': 54, 'belajar nlp': 10, 'nlp dan': 44, 'dan text': 17, 'text mining': 64, 'mining ternyata': 40, 'ternyata seru': 62, 'seru banget': 58, 'banget sadiezz': 8, 'sudah': 59, 'lumayan': 32, 'lama': 29, 'bingits': 14, 'tukang': 67, 'bakso': 4, 'belum': 11, 'lewat': 31, 'sudah lumayan': 60, 'lumayan lama': 33, 'lama bingits': 30, 'bingits tukang': 15, 'tukang bakso': 68, 'bakso belum': 5, 'belum lewat': 12, 'aduh': 0, 'ga': 19, 'mie': 37, 'ayam': 2, 'p4k4i': 45, 'kesyap': 25, 'please': 52, 'deh': 18, 'aduh ga': 1, 'ga banget': 20, 'banget makan': 7, 'makan mie': 35, 'mie ayam': 38, 'ayam p4k4i': 3, 'p4k4i kesyap': 46, 'kesyap please': 26, 'please deh': 53}\n"
          ]
        }
      ],
      "source": [
        "# N-Grams VSM\n",
        "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "vsm = tfidf_vectorizer.fit_transform(D)\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLWTtw4CNk0k",
        "outputId": "b2f76a71-07ac-4f99-dace-7abe738b05fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'seru banget': 0,\n",
              " 'seru': 1,\n",
              " 'the best': 2,\n",
              " 'lama': 3,\n",
              " 'text mining': 4,\n",
              " 'nlp': 5,\n",
              " 'ayam': 6}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vocabulary based VSM\n",
        "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
        "# variasi 2\n",
        "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
        "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
        "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat seru'\n",
        "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
        "D = [d1,d2,d3,d4]\n",
        "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
        "tf_vectorizer = CountVectorizer(binary = False, vocabulary=Vocab)\n",
        "tf = tf_vectorizer.fit_transform(D)\n",
        "print(tf.toarray())\n",
        "tf_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amBYKt-qNk0k",
        "outputId": "371fa4ac-b3a6-41a1-85fa-b38dde1696dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'seru banget': 0, 'the best': 1, 'lama': 2, 'text mining': 3, 'nlp': 4, 'ayam': 5}\n"
          ]
        }
      ],
      "source": [
        "Vocab = {'seru banget':0, 'the best':1, 'lama':2, 'text mining':3, 'nlp':4, 'ayam':5}\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, lowercase=True, vocabulary=Vocab)\n",
        "vsm = tfidf_vectorizer.fit_transform(D)\n",
        "print(tfidf_vectorizer.vocabulary_)\n",
        "# VSM terurut sesuai definisi dan terkesan lebih \"bersih\"\n",
        "# Perusahaan besar biasanya memiliki menggunakan teknik ini dengan vocabulary yang comprehensif\n",
        "# Sangat cocok untuk Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCLKJNu5Nk0k",
        "outputId": "be46dc3a-970f-44e9-cd22-da4817c9a7fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['aiceindonesia moga aksi sosial masyarakat indonesia disiplin pakai masker patuh protokol sehat',\n",
              " 'ecosocrights konglomerat kaya jabat pandemi kaya presiden pandemi menguntungka',\n",
              " 'temen temen yg kaya pandemi']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSqOEmsANk0k",
        "outputId": "f80debcb-7bf3-4342-c490-8f8a84fa7361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['aiceindonesia', 'moga', 'aksi', 'sosial', 'masyarakat', 'indonesia', 'disiplin', 'pakai', 'masker', 'patuh', 'protokol', 'sehat'], ['ecosocrights', 'konglomerat', 'kaya', 'jabat', 'pandemi', 'kaya', 'presiden', 'pandemi', 'menguntungka'], ['temen', 'temen', 'yg', 'kaya', 'pandemi']]\n"
          ]
        }
      ],
      "source": [
        "# Rubah bentuk data seperti yang dibutuhkan genSim\n",
        "# Bisa juga dilakukan dengan memodifikasi fungsi \"cleanText\" (agar lebih efisien)\n",
        "\n",
        "data_we = []\n",
        "for doc in data:\n",
        "    Tokens = [str(w) for w in TextBlob(doc).words]\n",
        "    data_we.append(Tokens)\n",
        "print(data_we[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcOgW3_uNk0l",
        "outputId": "7f41fb2c-18cc-4d1b-fd74-0aff3c16b136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!...\n"
          ]
        }
      ],
      "source": [
        "# https://radimrehurek.com/gensim/models/word2vec.html\n",
        "# train word2vec dengan data di atas\n",
        "\n",
        "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
        "model_wv = Word2Vec(data_we, min_count=2, vector_size=L, window = 5, workers= -2)\n",
        "# min_count adalah jumlah kata minimal yang muncul di corpus\n",
        "# \"vector_size\" adalah Dimensionality of the word vectors \n",
        "# (menurut beberapa literature untuk text disarankan 300-500)\n",
        "# \"window\" adalah jarak maximum urutan kata yang di pertimbangkan\n",
        "# workers = jumlah prosesor yang digunakan untuk menjalankan word2vec\n",
        "print('Done!...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heyNk2EiNk0l",
        "outputId": "10ef5152-d1a9-42ea-cf87-d7fac5542df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!...\n"
          ]
        }
      ],
      "source": [
        "# di data yang sebenarnya (i.e. besar) Gensim sering membutuhkan waktu cukup lama\n",
        "# Untungnya kita bisa menyimpan dan me-load kembali hasil perhitungan model word2vec, misal\n",
        "model_wv.save('data/model_w2v')\n",
        "model_wv = Word2Vec.load('data/model_w2v')\n",
        "print('Done!...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgQMFy_bNk0l"
      },
      "source": [
        "### Hati-hati, Word2vec menggunakan Matriks Dense\n",
        "\n",
        "<p>Penggunaan memory oleh Gensim kurang lebih sebagai berikut:</p>\n",
        "\n",
        "<p>Jumlah kata x &quot;size&quot; x 12 bytes</p>\n",
        "\n",
        "<p>Misal terdapat 100 000 kata unik dan menggunakan 200 layers, maka penggunaan memory =&nbsp;</p>\n",
        "\n",
        "<p>100,000x200x12 bytes = ~229MB</p>\n",
        "\n",
        "<p>Jika jumlah size semakin banyak, maka jumlah training data yang diperlukan juga semakin banyak, namun model akan semakin akurat.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yjGrwx-Nk0l",
        "outputId": "51116d87-6748-4771-a628-a60de62b6163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n",
            "[-0.00139513 -0.00099477  0.00258718  0.00241652 -0.00276821]\n"
          ]
        }
      ],
      "source": [
        "# Melihat vector suatu kata\n",
        "vektor = model_wv.wv.__getitem__(['psbb'])\n",
        "print(len(vektor[0])) # Panjang vektor keseluruhan = jumlah neuron yang digunakan\n",
        "print(vektor[0][:5]) # 5 elemen pertama dari vektornya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40zo6PzuNk0l",
        "outputId": "e0dfab6b-f9ed-42f1-cb8e-c1e4d42ece53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('kontannews', 0.21803781390190125),\n",
              " ('muhammad', 0.2029067873954773),\n",
              " ('bela', 0.19181834161281586),\n",
              " ('biang', 0.1762441098690033),\n",
              " ('stiap', 0.17516076564788818),\n",
              " ('teori', 0.1729869544506073),\n",
              " ('manajemen', 0.1714370846748352),\n",
              " ('mensos', 0.16922974586486816),\n",
              " ('aktivitas', 0.16919642686843872),\n",
              " ('ppkn', 0.16815714538097382)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mencari kata terdekat menurut data training dan Word2Vec\n",
        "model_wv.wv.most_similar('psbb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-zJ4XA3Nk0l",
        "outputId": "f3b25846-5bbc-4db4-c9f1-b3fd7695769a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.062678315\n",
            "-0.039310396\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# Melihat similarity antar kata\n",
        "print(model_wv.wv.similarity('psbb', 'corona'))\n",
        "print(model_wv.wv.similarity('psbb', 'bioskop'))\n",
        "print(model_wv.wv.similarity('psbb', 'psbb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv3AdTRPNk0l"
      },
      "source": [
        "## Hati-hati Cosine adalah similarity bukan distance\n",
        "Hal ini akan mempengaruhi interpretasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qWvQ5qwNk0l",
        "outputId": "c0bb6176-52bb-48f0-84de-65702731cfef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error! kata \" copid \" tidak ada di training data\n"
          ]
        }
      ],
      "source": [
        "# error jika kata tidak ada di training data\n",
        "# beckman bukan beckmans ==> hence di Word Embedding PreProcessing harus thourough\n",
        "\n",
        "kata = 'copid'\n",
        "try:\n",
        "    print(model_wv.wv.most_similar(kata))\n",
        "except:\n",
        "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
        "# ini salah satu kelemahan Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qGuxD9UNk0m"
      },
      "source": [
        "## Tips:\n",
        "\n",
        "<p>Hati-hati GenSim tidak menggunakan seluruh kata di training data!.</p>\n",
        "\n",
        "<p>Perintah berikut akan menghasilkan kata-kata yang terdapat di vocabulary GenSim</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s7_KPI7Nk0m",
        "outputId": "995bc468-cdc3-415a-e643-cb40f22b4dbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pandemi',\n",
              " 'yg',\n",
              " 'kaya',\n",
              " 'bantu',\n",
              " 'rp',\n",
              " 'miliar',\n",
              " 'harta',\n",
              " 'dampak',\n",
              " 'ga',\n",
              " 'luhut',\n",
              " 'jabat',\n",
              " 'musisi',\n",
              " 'pandjaitan',\n",
              " 'binsar',\n",
              " 'ekonomi',\n",
              " 'kuliah',\n",
              " 'oposisicerdas',\n",
              " 'ya',\n",
              " 'jokowi',\n",
              " 'jatengopini',\n",
              " 'orang',\n",
              " 'jawa',\n",
              " 'halo',\n",
              " 'lho',\n",
              " 'masyarakat',\n",
              " 'kreatif',\n",
              " 'gak',\n",
              " 'manggung',\n",
              " 'tangis',\n",
              " 'tapi',\n",
              " 'anies',\n",
              " 'rakyat',\n",
              " 'turun']"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampel_ = 33\n",
        "model_wv.wv.index_to_key[:sampel_]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgvgWC-9Nk0m"
      },
      "source": [
        "## Hati-hati menginterpretasikan hasil Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfr_hdbYNk0m"
      },
      "source": [
        "<h3 id=\"Â FastText-(Facebook-2016)\">&nbsp;FastText (Facebook-2016)</h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
        "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
        "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
        "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8x93aydNk0q",
        "outputId": "3e156ed0-066e-4cae-bf6a-8d80b956f577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Done'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Caution penggunaan memory besar, bila timbul \"Memory Error\" kecilkan nilai L\n",
        "\n",
        "L = 100 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
        "model_FT = FastText(data_we, vector_size=L, window=5, min_count=2, workers=-2)\n",
        "'Done'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HESH7nfGNk0q",
        "outputId": "774de45e-79b3-4b74-85b3-66d0880fca3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('naek', 0.34502407908439636),\n",
              " ('mek', 0.3286518454551697),\n",
              " ('home', 0.3269801735877991),\n",
              " ('bulan', 0.31775084137916565),\n",
              " ('blm', 0.3159526288509369),\n",
              " ('kapolres', 0.3140672445297241),\n",
              " ('himpun', 0.3126278817653656),\n",
              " ('psi', 0.3098238706588745),\n",
              " ('himperaktif', 0.30772924423217773),\n",
              " ('cipedes', 0.30604347586631775)]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mencari kata terdekat menurut data training dan Word2Vec\n",
        "model_FT.wv.most_similar('psbb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s_YNlAMNk0r",
        "outputId": "accb956e-ece6-4633-ffbe-73e7dbe0759e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.082103185\n",
            "-0.1547481\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# Melihat similarity antar kata\n",
        "print(model_FT.wv.similarity('psbb', 'corona'))\n",
        "print(model_FT.wv.similarity('psbb', 'jakarta'))\n",
        "print(model_FT.wv.similarity('psbb', 'psbb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d8qrqZJNk0r",
        "outputId": "9de2eafd-9d96-411b-e33d-51576389376b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec error!\n",
            "[('corona', 0.5362619161605835), ('coronavirus', 0.4721188545227051), ('narsih', 0.3698681890964508), ('panggil', 0.3449081778526306), ('bersih', 0.3184361159801483), ('coronce', 0.31669026613235474), ('jepit', 0.3149551451206207), ('orgil', 0.3126393258571625), ('sih', 0.3084302246570587), ('siap', 0.3081304430961609)]\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec VS FastText\n",
        "try:\n",
        "    print(model_wv.wv.most_similar('coro'))\n",
        "except:\n",
        "    print('Word2Vec error!')\n",
        "    \n",
        "try:\n",
        "    print(model_FT.wv.most_similar('coro'))\n",
        "except:\n",
        "    print('FastText error!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkpZ3f6INk0r"
      },
      "source": [
        "# Diskusi:\n",
        "<ul>\n",
        "\t<li>Apakah kelebihan dan kekurangan WE secara umum?</li>\n",
        "\t<li>Apakah kira-kira aplikasi WE?</li>\n",
        "\t<li>Apakah bisa dijadikan representasi dokumen? Bagaimana caranya?</li>\n",
        "\t<li>Bergantung pada apa sajakah performa model WE?</li>\n",
        "</ul>\n",
        "\n",
        "* Preprocessing apa yang sebaiknya dilakukan pada model Word Embedding?\n",
        "* Apakah Pos Tag bermanfaat disini? Jika iya bagaimana menggunakannya?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ-8TZ4ANk0r"
      },
      "source": [
        "<h1>End of Module</h1>\n",
        "<hr />\n",
        "<p><img alt=\"\" src=\"https://github.com/taudataid/eLearning/blob/master/images/meme-cartoon/2_Studying_Linguistic.png?raw=1\" style=\"height:500px; width:667px\" /></p>\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}